In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.
Recent work has achieved significant improvements in computational efficiency through factorization tricks  and conditional computation , while also improving model performance in case of the latter.
To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.
In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section ..
The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key
Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences .
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.
At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next.
Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
Each layer has two sub-layers.
We employ a residual connection  around each of the two sub-layers, followed by layer normalization .
Recurrent models typically factor computation along the symbol positions of the input and output sequences.
â€¡Work performed while at Google Research.
This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.
Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensortensor.
days on eight GPUs, a small fraction of the training costs of the best models from the literature.
